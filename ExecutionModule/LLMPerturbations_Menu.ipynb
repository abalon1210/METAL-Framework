{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import google.generativeai as palm\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc547d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key='YOUR_GooglePaLM_API_KEY_HERE')\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"YOUR_OPENAI_API_KEY_HERE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c096ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "# GooglePaLM\n",
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model = models[0].name\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7befdbf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#ChatGPT\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions.py:594\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "#ChatGPT\n",
    "completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"hello\"}])\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d345100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talktomodel(prompt, modelName):\n",
    "\n",
    "    if modelName == \"LLaMa2\":\n",
    "        args = (\"../../llama.cpp/main\", \"-m\", \"../../llama.cpp/models/llama2-7b-chat/llama-2-7b-chat.Q4_0.gguf\", \"-p\", prompt, \"-n\", \"2048\")\n",
    "\n",
    "        temp = subprocess.Popen(args, stdout = subprocess.PIPE)\n",
    "        output = str(temp.communicate())\n",
    "\n",
    "        return output\n",
    "\n",
    "    elif modelName == \"ChatGPT\":\n",
    "        completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "        \n",
    "    else:\n",
    "        completion = palm.generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        # The maximum length of the response\n",
    "        max_output_tokens=800,\n",
    "        )\n",
    "\n",
    "        return(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1658f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_csv(i, task, pertModel, modelName, inputID, inputText, pertMethod, pertText, origOut, pertOut, origTime, pertTime):\n",
    "    csvName = 'LLMFunctionsBy' + pertModel + '_Iteration' + str(i) + '_' + task + '_' + modelName + '.csv'\n",
    "    \n",
    "    if not os.path.isfile(csvName) or os.path.getsize(csvName) == 0:\n",
    "        with open(csvName, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(['InputTextID', 'InputText', 'PerturbationID', 'PerturbedText', 'OriginalOutput', 'PerturbedOutput', 'OriginalTime', 'PerturbedTime'])\n",
    "    \n",
    "    with open(csvName, 'a', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([inputID, inputText, pertMethod, pertText, origOut, pertOut, origTime, pertTime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a791d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_typos(input_text, modelName):\n",
    "    prompt = \"Please add many typos for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_characters(input_text, modelName):\n",
    "    prompt = \"Please delete alphabets randomly for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa73e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_characters(input_text, modelName):\n",
    "    prompt = \"Please shuffle the characters of random words for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a04b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_characters(input_text, modelName):\n",
    "    prompt = \"Please add random english alphabet characters for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8100525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_leet(input_text, modelName):\n",
    "    prompt = \"Please convert random words for each sentence in this text to l33t format, where a,e,i,o is replaced by 4,3,1,0 respectively.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54354b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces(input_text, modelName):\n",
    "    prompt = \"Please add lots of spaces randomly for each sentence in-between this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c3956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words(input_text, modelName):\n",
    "    prompt = \"Please add random words for each sentence in this text. Choose from the following: Apple, Pear, Banana, Grape\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb25a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_characters(input_text, modelName):\n",
    "    prompt = \"Please swap any two characters in random words for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77642c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym_replacement(input_text, modelName):\n",
    "    prompt = \"Please give me text that is semantically opposite to each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ac1ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(input_text, modelName):\n",
    "    prompt = \"Please paraphrase the each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dca8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_random_sentence(input_text, modelName):\n",
    "    prompt = \"Please replace random sentences in this text with this sentence: Lorem ipsum dolor sit amet.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c327c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_sentence(input_text, modelName):\n",
    "    prompt = \"Plrease remove random sentences from this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f809e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_mapping = {\n",
    "    \"sentiment analysis\": \"Analyse the sentiment of this text as positive, negative or neutral.\",\n",
    "    \"text summarization\": \"Summarize this text in five sentences.\",\n",
    "    \"information retrieval\": \"List the top 10 information from this text.\",\n",
    "    \"toxicity detection\": \"Check whether this text contains toxic or spam content and say yes, no or unknown. Also provide reasons.\",\n",
    "    \"news classification\": \"Categorize the news text into the following categories: World, Sports, Business, Science/Technology.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65605a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, iteration, modelName, pertModel):\n",
    "    # add_to_csv(iteration, task, pertModel, modelName, inputID, inputText, pertMethod, pertText, origOut, pertOut, origTime, pertTime)\n",
    "    task = row['task']\n",
    "    input_text = row['input']\n",
    "    inputID = row['inputID']\n",
    "    time.sleep(1.0)\n",
    "    if task in prompt_mapping:\n",
    "        \n",
    "        if task == \"sentiment analysis\" or task == \"toxicity detection\" or task == \"news classification\":\n",
    "            prompt = prompt_mapping[task]\n",
    "            \n",
    "            orig_new_text = input_text + \" \" + prompt\n",
    "            start_time = time.time()\n",
    "            origOut = talktomodel(orig_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            origTime = end_time - start_time\n",
    "        \n",
    "            perturbed_text = introduce_typos(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Introducing Typos\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = delete_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Deleting Characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = synonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing Synonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = add_words(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random words\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = to_leet(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Converting to l33t format\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = shuffle_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Shuffling characters in each word\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = add_spaces(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding spaces in text\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = add_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = swap_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Swapping two random characters in each word\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = antonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing words with their antonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        elif task == \"text summarization\" or task == \"information retrieval\":\n",
    "            prompt = prompt_mapping[task]\n",
    "            \n",
    "            orig_new_text = input_text + \" \" + prompt\n",
    "            start_time = time.time()\n",
    "            origOut = talktomodel(orig_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            origTime = end_time - start_time\n",
    "        \n",
    "            perturbed_text = introduce_typos(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Introducing Typos\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = delete_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Deleting Characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = synonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing Synonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = add_words(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random words\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "     \n",
    "            perturbed_text = replace_random_sentence(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing random sentences\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = remove_random_sentence(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Removing random sentences\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = to_leet(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Converting to l33t format\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = add_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = antonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing words with their antonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = shuffle_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Shuffling characters in each word\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "    else:\n",
    "        # task = question answering\n",
    "        orig_new_text = input_text\n",
    "        start_time = time.time()\n",
    "        origOut = talktomodel(orig_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        origTime = end_time - start_time\n",
    "        \n",
    "        pert_new_text = introduce_typos(input_text, pertModel)\n",
    "        pertMethod = \"Introducing Typos\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = delete_characters(input_text, pertModel)\n",
    "        pertMethod = \"Deleting Characters\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = synonym_replacement(input_text, pertModel)\n",
    "        pertMethod = \"Replacing Synonyms\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = add_words(input_text, pertModel)\n",
    "        pertMethod = \"Adding random words\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = to_leet(input_text, pertModel)\n",
    "        pertMethod = \"Converting to l33t format\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = shuffle_characters(input_text, pertModel)\n",
    "        pertMethod = \"Shuffling characters in each word\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = add_spaces(input_text, pertModel)\n",
    "        pertMethod = \"Adding spaces in text\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = add_characters(input_text, pertModel)\n",
    "        pertMethod = \"Adding random characters\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = swap_characters(input_text, pertModel)\n",
    "        pertMethod = \"Swapping two random characters in each word\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = antonym_replacement(input_text, pertModel)\n",
    "        pertMethod = \"Replacing words with their antonyms\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c40fd6-841a-4ecf-81d3-98d6f2959a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(pertModelChoice, targetModelChoice, task_choice, num_iters):\n",
    "    if str(pertModelChoice) == '2':\n",
    "        pertModel = \"ChatGPT\"\n",
    "    elif str(pertModelChoice) == '3':\n",
    "        pertModel = \"LLaMa2\"\n",
    "    else:\n",
    "        pertModel = \"GooglePaLM\"\n",
    "\n",
    "    if str(targetModelChoice) == '2':\n",
    "        targetModel = \"ChatGPT\"\n",
    "    elif str(targetModelChoice) == '3':\n",
    "        targetModel = \"LLaMa2\"\n",
    "    else:\n",
    "        targetModel = \"GooglePaLM\"\n",
    "\n",
    "    print(\"Testing target model: \" + targetModel + \" with perturbations by: \" + pertModel)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if str(task_choice) == '1':\n",
    "        task = \"Information Retrieval\"\n",
    "        csv_file_path = 'Robustness/ir_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '2':\n",
    "        task = \"News Classification\"\n",
    "        csv_file_path = 'Robustness/nc_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '3':\n",
    "        task = \"Question Answering\"\n",
    "        csv_file_path = 'Robustness/qa_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '5':\n",
    "        task = \"Toxicity Detection\"\n",
    "        csv_file_path = 'Robustness/td_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '6':\n",
    "        task = \"Text Summarization\"\n",
    "        csv_file_path = 'Robustness/ts_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    else:\n",
    "        task = \"Sentiment Analysis\"\n",
    "        csv_file_path = 'Robustness/sa_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    print(\"Testing Task: \" + task)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for i in range(0, int(num_iters)):\n",
    "        for index, row in df.iterrows():\n",
    "            process_row(row, i, targetModel, pertModel)\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47a45fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Welcome to the METAL Framework - Test LLMs using Metamorphic Testing\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Please note that due to LLM limitations, only the Robustness QA can be tested using LLM-based perturbations\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    pert_model_choice = input(\"Choose the model you want to generate perturbations with. Enter 1 for Google PaLM, 2 for ChatGPT, 3 for LLaMa2 (Defaults to Google PaLM). Use comma-separated values for multiple options (Eg. 1,2): \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    pert_model_choice = pert_model_choice.strip()\n",
    "    pert_model_choices = pert_model_choice.split(',')\n",
    "\n",
    "    for pmc in pert_model_choices:\n",
    "        model_choice = input(\"Choose the model you want to test. Enter 1 for Google PaLM, 2 for ChatGPT, 3 for LLaMa2 (Defaults to Google PaLM). Use comma-separated values for multiple options (Eg. 1,2): \")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        model_choice = model_choice.strip()\n",
    "        model_choices = model_choice.split(',')\n",
    "\n",
    "        for mc in model_choices:\n",
    "            task_choice = input(\"Choose the Robustness task you want to test the model against. Enter 1 for Information Retrieval, 2 for News Classification, 3 for Question Answering, 4 for Sentiment Analysis, 5 for Toxicity Detection, 6 for Text Summarization (Defaults to Sentiment Analysis). Use comma-separated values for multiple options (Eg. 1,2): \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            task_choice = task_choice.strip()\n",
    "            task_choices = task_choice.split(',')\n",
    "\n",
    "            for tc in task_choices:\n",
    "                num_iters = input(\"Enter the number of iterations you want task \" + str(tc) + \" to run (Limit to 100, defaults to 10): \")\n",
    "                print(\"\\n\")\n",
    "                try:\n",
    "                    user_input = int(num_iters)\n",
    "\n",
    "                    if not 1 <= user_input <= 100:\n",
    "                        print(\"Number of iterations outside range. Defaulting to 10 iterations\")\n",
    "                        print(\"\\n\")\n",
    "                        num_iters = 10\n",
    "\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Defaulting to 10 iterations.\")\n",
    "                    print(\"\\n\")\n",
    "                    num_iters = 10\n",
    "\n",
    "                run(pmc, mc, tc, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b055a4-9100-44b0-9dae-406996477052",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
