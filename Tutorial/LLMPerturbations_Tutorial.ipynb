{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import google.generativeai as palm\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc547d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key='YOUR-GOOGLEPALM-API-KEY-HERE')\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"YOUR_OPENAI_API_KEY_HERE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c096ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "# GooglePaLM\n",
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model = models[0].name\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d345100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talktomodel(prompt, modelName):\n",
    "\n",
    "    if modelName == \"LLaMa2\":\n",
    "        args = (\"../../llama.cpp/main\", \"-m\", \"../../llama.cpp/models/llama2-7b-chat/llama-2-7b-chat.Q4_0.gguf\", \"-p\", prompt, \"-n\", \"2048\")\n",
    "\n",
    "        temp = subprocess.Popen(args, stdout = subprocess.PIPE)\n",
    "        output = str(temp.communicate())\n",
    "\n",
    "        return output\n",
    "\n",
    "    elif modelName == \"ChatGPT\":\n",
    "        completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "        \n",
    "    else:\n",
    "        completion = palm.generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        # The maximum length of the response\n",
    "        max_output_tokens=800,\n",
    "        )\n",
    "\n",
    "        return(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1658f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_csv(i, task, pertModel, modelName, inputID, inputText, pertMethod, pertText, origOut, pertOut, origTime, pertTime):\n",
    "    csvName = 'LLMFunctionsBy' + pertModel + '_Iteration' + str(i) + '_' + task + '_' + modelName + '.csv'\n",
    "    \n",
    "    if not os.path.isfile(csvName) or os.path.getsize(csvName) == 0:\n",
    "        with open(csvName, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(['InputTextID', 'InputText', 'PerturbationID', 'PerturbedText', 'OriginalOutput', 'PerturbedOutput', 'OriginalTime', 'PerturbedTime'])\n",
    "    \n",
    "    with open(csvName, 'a', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([inputID, inputText, pertMethod, pertText, origOut, pertOut, origTime, pertTime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a791d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_typos(input_text, modelName):\n",
    "    prompt = \"Please add many typos for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_characters(input_text, modelName):\n",
    "    prompt = \"Please delete alphabets randomly for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa73e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_characters(input_text, modelName):\n",
    "    prompt = \"Please shuffle the characters of random words for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a04b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_characters(input_text, modelName):\n",
    "    prompt = \"Please add random english alphabet characters for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8100525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_leet(input_text, modelName):\n",
    "    prompt = \"Please convert random words for each sentence in this text to l33t format, where a,e,i,o is replaced by 4,3,1,0 respectively.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54354b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces(input_text, modelName):\n",
    "    prompt = \"Please add lots of spaces randomly for each sentence in-between this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c3956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words(input_text, modelName):\n",
    "    prompt = \"Please add random words for each sentence in this text. Choose from the following: Apple, Pear, Banana, Grape\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb25a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_characters(input_text, modelName):\n",
    "    prompt = \"Please swap any two characters in random words for each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77642c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym_replacement(input_text, modelName):\n",
    "    prompt = \"Please give me text that is semantically opposite to each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ac1ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(input_text, modelName):\n",
    "    prompt = \"Please paraphrase the each sentence in this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dca8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_random_sentence(input_text, modelName):\n",
    "    prompt = \"Please replace random sentences in this text with this sentence: Lorem ipsum dolor sit amet.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c327c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_sentence(input_text, modelName):\n",
    "    prompt = \"Plrease remove random sentences from this text.\"\n",
    "    new_input = \"'\" + input_text + \"' \" + prompt\n",
    "    model_output = talktomodel(new_input, modelName)\n",
    "    \n",
    "    if model_output:\n",
    "        return model_output\n",
    "    else:\n",
    "        return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f809e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_mapping = {\n",
    "    \"sentiment analysis\": \"Analyse the sentiment of this text as positive, negative or neutral.\",\n",
    "    \"text summarization\": \"Summarize this text in five sentences.\",\n",
    "    \"information retrieval\": \"List the top 10 information from this text.\",\n",
    "    \"toxicity detection\": \"Check whether this text contains toxic or spam content and say yes, no or unknown. Also provide reasons.\",\n",
    "    \"news classification\": \"Categorize the news text into the following categories: World, Sports, Business, Science/Technology.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65605a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, iteration, modelName, pertModel):\n",
    "    # add_to_csv(iteration, task, pertModel, modelName, inputID, inputText, pertMethod, pertText, origOut, pertOut, origTime, pertTime)\n",
    "    task = row['task']\n",
    "    input_text = row['input']\n",
    "    inputID = row['inputID']\n",
    "    time.sleep(1.0)\n",
    "    if task in prompt_mapping:\n",
    "        \n",
    "        if task == \"sentiment analysis\" or task == \"toxicity detection\" or task == \"news classification\":\n",
    "            prompt = prompt_mapping[task]\n",
    "            \n",
    "            orig_new_text = input_text + \" \" + prompt\n",
    "            start_time = time.time()\n",
    "            origOut = talktomodel(orig_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            origTime = end_time - start_time\n",
    "        \n",
    "            perturbed_text = introduce_typos(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Introducing Typos\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = delete_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Deleting Characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = synonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing Synonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = add_words(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random words\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = to_leet(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Converting to l33t format\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = shuffle_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Shuffling characters in each word\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = add_spaces(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding spaces in text\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = add_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = swap_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Swapping two random characters in each word\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = antonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing words with their antonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        elif task == \"text summarization\" or task == \"information retrieval\":\n",
    "            prompt = prompt_mapping[task]\n",
    "            \n",
    "            orig_new_text = input_text + \" \" + prompt\n",
    "            start_time = time.time()\n",
    "            origOut = talktomodel(orig_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            origTime = end_time - start_time\n",
    "        \n",
    "            perturbed_text = introduce_typos(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Introducing Typos\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = delete_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Deleting Characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = synonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing Synonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "            perturbed_text = add_words(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random words\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "     \n",
    "            perturbed_text = replace_random_sentence(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing random sentences\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = remove_random_sentence(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Removing random sentences\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = to_leet(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Converting to l33t format\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = add_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Adding random characters\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = antonym_replacement(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Replacing words with their antonyms\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "            perturbed_text = shuffle_characters(input_text, pertModel)\n",
    "            pert_new_text = perturbed_text + \" \" + prompt\n",
    "            pertMethod = \"Shuffling characters in each word\"\n",
    "            start_time = time.time()\n",
    "            pertOut = talktomodel(pert_new_text, modelName)\n",
    "            end_time = time.time()\n",
    "            pertTime = end_time - start_time\n",
    "            add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "    else:\n",
    "        # task = question answering\n",
    "        orig_new_text = input_text\n",
    "        start_time = time.time()\n",
    "        origOut = talktomodel(orig_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        origTime = end_time - start_time\n",
    "        \n",
    "        pert_new_text = introduce_typos(input_text, pertModel)\n",
    "        pertMethod = \"Introducing Typos\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = delete_characters(input_text, pertModel)\n",
    "        pertMethod = \"Deleting Characters\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = synonym_replacement(input_text, pertModel)\n",
    "        pertMethod = \"Replacing Synonyms\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = add_words(input_text, pertModel)\n",
    "        pertMethod = \"Adding random words\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "        \n",
    "        pert_new_text = to_leet(input_text, pertModel)\n",
    "        pertMethod = \"Converting to l33t format\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = shuffle_characters(input_text, pertModel)\n",
    "        pertMethod = \"Shuffling characters in each word\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = add_spaces(input_text, pertModel)\n",
    "        pertMethod = \"Adding spaces in text\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = add_characters(input_text, pertModel)\n",
    "        pertMethod = \"Adding random characters\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = swap_characters(input_text, pertModel)\n",
    "        pertMethod = \"Swapping two random characters in each word\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n",
    "            \n",
    "        pert_new_text = antonym_replacement(input_text, pertModel)\n",
    "        pertMethod = \"Replacing words with their antonyms\"\n",
    "        start_time = time.time()\n",
    "        pertOut = talktomodel(pert_new_text, modelName)\n",
    "        end_time = time.time()\n",
    "        pertTime = end_time - start_time\n",
    "        add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c40fd6-841a-4ecf-81d3-98d6f2959a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(pertModelChoice, targetModelChoice, task_choice, num_iters):\n",
    "    if str(pertModelChoice) == '2':\n",
    "        pertModel = \"ChatGPT\"\n",
    "    elif str(pertModelChoice) == '3':\n",
    "        pertModel = \"LLaMa2\"\n",
    "    else:\n",
    "        pertModel = \"GooglePaLM\"\n",
    "\n",
    "    if str(targetModelChoice) == '2':\n",
    "        targetModel = \"ChatGPT\"\n",
    "    elif str(targetModelChoice) == '3':\n",
    "        targetModel = \"LLaMa2\"\n",
    "    else:\n",
    "        targetModel = \"GooglePaLM\"\n",
    "\n",
    "    print(\"Testing target model: \" + targetModel + \" with perturbations by: \" + pertModel)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if str(task_choice) == '1':\n",
    "        task = \"Information Retrieval\"\n",
    "        csv_file_path = 'Robustness/ir_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '2':\n",
    "        task = \"News Classification\"\n",
    "        csv_file_path = 'Robustness/nc_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '3':\n",
    "        task = \"Question Answering\"\n",
    "        csv_file_path = 'Robustness/qa_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '5':\n",
    "        task = \"Toxicity Detection\"\n",
    "        csv_file_path = 'Robustness/td_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    elif str(task_choice) == '6':\n",
    "        task = \"Text Summarization\"\n",
    "        csv_file_path = 'Robustness/ts_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    else:\n",
    "        task = \"Sentiment Analysis\"\n",
    "        csv_file_path = 'Robustness/sa_inputs.csv'\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    print(\"Testing Task: \" + task)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for i in range(0, int(num_iters)):\n",
    "        for index, row in df.iterrows():\n",
    "            process_row(row, i, targetModel, pertModel)\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a45fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Welcome to the METAL Framework - Test LLMs using Metamorphic Testing\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Please note that due to LLM limitations, only the Robustness QA can be tested using LLM-based perturbations\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # pert_model_choice = input(\"Choose the model you want to generate perturbations with. Enter 1 for Google PaLM, 2 for ChatGPT, 3 for LLaMa2 (Defaults to Google PaLM). Use comma-separated values for multiple options (Eg. 1,2): \")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    print(\"Using Model 1 - GooglePaLM to create perturbations\")\n",
    "    print(\"\\n\")\n",
    "    pert_model_choice = '1'\n",
    "\n",
    "    pert_model_choice = pert_model_choice.strip()\n",
    "    pert_model_choices = pert_model_choice.split(',')\n",
    "\n",
    "    for pmc in pert_model_choices:\n",
    "        # model_choice = input(\"Choose the model you want to test. Enter 1 for Google PaLM, 2 for ChatGPT, 3 for LLaMa2 (Defaults to Google PaLM). Use comma-separated values for multiple options (Eg. 1,2): \")\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        print(\"Using Model 1 - GooglePaLM to test\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        model_choice = '1'\n",
    "\n",
    "        model_choice = model_choice.strip()\n",
    "        model_choices = model_choice.split(',')\n",
    "\n",
    "        for mc in model_choices:\n",
    "            task_choice = input(\"Choose the Robustness task you want to test the model against. Enter 1 for Information Retrieval, 2 for News Classification, 3 for Question Answering, 4 for Sentiment Analysis, 5 for Toxicity Detection, 6 for Text Summarization (Defaults to Sentiment Analysis). Use comma-separated values for multiple options (Eg. 1,2): \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            task_choice = task_choice.strip()\n",
    "            task_choices = task_choice.split(',')\n",
    "\n",
    "            for tc in task_choices:\n",
    "                num_iters = input(\"Enter the number of iterations you want task \" + str(tc) + \" to run (Limit to 100, defaults to 10): \")\n",
    "                print(\"\\n\")\n",
    "                try:\n",
    "                    user_input = int(num_iters)\n",
    "\n",
    "                    if not 1 <= user_input <= 100:\n",
    "                        print(\"Number of iterations outside range. Defaulting to 10 iterations\")\n",
    "                        print(\"\\n\")\n",
    "                        num_iters = 10\n",
    "\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Defaulting to 10 iterations.\")\n",
    "                    print(\"\\n\")\n",
    "                    num_iters = 10\n",
    "\n",
    "                run(pmc, mc, tc, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22b055a4-9100-44b0-9dae-406996477052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the METAL Framework - Test LLMs using Metamorphic Testing\n",
      "\n",
      "\n",
      "Please note that due to LLM limitations, only the Robustness QA can be tested using LLM-based perturbations\n",
      "\n",
      "\n",
      "Using Model 1 - GooglePaLM to create perturbations\n",
      "\n",
      "\n",
      "Using Model 1 - GooglePaLM to test\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the Robustness task you want to test the model against. Enter 1 for Information Retrieval, 2 for News Classification, 3 for Question Answering, 4 for Sentiment Analysis, 5 for Toxicity Detection, 6 for Text Summarization (Defaults to Sentiment Analysis). Use comma-separated values for multiple options (Eg. 1,2):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of iterations you want task 4 to run (Limit to 100, defaults to 10):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing target model: GooglePaLM with perturbations by: GooglePaLM\n",
      "\n",
      "\n",
      "Testing Task: Sentiment Analysis\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/grpc_helpers.py:79\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/grpc/_channel.py:1160\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1154\u001b[0m (\n\u001b[1;32m   1155\u001b[0m     state,\n\u001b[1;32m   1156\u001b[0m     call,\n\u001b[1;32m   1157\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1158\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1159\u001b[0m )\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/grpc/_channel.py:1003\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.76.106:443 {created_time:\"2024-03-27T09:20:42.588468+10:30\", grpc_status:13, grpc_message:\"An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 53\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     num_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 54\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(pertModelChoice, targetModelChoice, task_choice, num_iters)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(num_iters)):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 54\u001b[0m         \u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpertModel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 81\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row, iteration, modelName, pertModel)\u001b[0m\n\u001b[1;32m     78\u001b[0m pertTime \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     79\u001b[0m add_to_csv(iteration, task, pertModel, modelName, inputID, orig_new_text, pertMethod, pert_new_text, origOut, pertOut, origTime, pertTime)\n\u001b[0;32m---> 81\u001b[0m perturbed_text \u001b[38;5;241m=\u001b[39m \u001b[43madd_characters\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpertModel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m pert_new_text \u001b[38;5;241m=\u001b[39m perturbed_text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prompt\n\u001b[1;32m     83\u001b[0m pertMethod \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding random characters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36madd_characters\u001b[0;34m(input_text, modelName)\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add random english alphabet characters for each sentence in this text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m new_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m input_text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prompt\n\u001b[0;32m----> 4\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mtalktomodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelName\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_output:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mtalktomodel\u001b[0;34m(prompt, modelName)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mpalm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The maximum length of the response\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(completion\u001b[38;5;241m.\u001b[39mresult)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/generativeai/text.py:200\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, prompt, temperature, candidate_count, max_output_tokens, top_p, top_k, safety_settings, stop_sequences, client)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the API and returns a `types.Completion` containing the response.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    A `types.Completion` containing the model's text completion response.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m request \u001b[38;5;241m=\u001b[39m _make_generate_text_request(\n\u001b[1;32m    189\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    190\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     stop_sequences\u001b[38;5;241m=\u001b[39mstop_sequences,\n\u001b[1;32m    198\u001b[0m )\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_generate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/generativeai/text.py:232\u001b[0m, in \u001b[0;36m_generate_response\u001b[0;34m(request, client)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_default_text_client()\n\u001b[0;32m--> 232\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;241m.\u001b[39mto_dict(response)\n\u001b[1;32m    235\u001b[0m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m safety_types\u001b[38;5;241m.\u001b[39mconvert_filters_to_enums(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py:648\u001b[0m, in \u001b[0;36mTextServiceClient.generate_text\u001b[0;34m(self, request, model, prompt, temperature, candidate_count, max_output_tokens, top_p, top_k, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    643\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    644\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmodel),)),\n\u001b[1;32m    645\u001b[0m )\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/retry.py:372\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    371\u001b[0m )\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/retry.py:207\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    209\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/grpc_helpers.py:81\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85871b-ad50-43df-9244-b09b58ea0b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
